{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pickle\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import json\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.util import mark_negation\n",
    "import nltk\n",
    "import os\n",
    "import utils\n",
    "from gensim.models import Word2Vec,FastText\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir=\"data/dataset/\"\n",
    "pretrained_vectors='data/GoogleNews-vectors-negative300.bin.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/dataset/text_3.txt\n",
      "data/dataset/text_7.txt\n",
      "data/dataset/text_2.txt\n",
      "data/dataset/text_11.txt\n",
      "data/dataset/text_8.txt\n",
      "data/dataset/text_6.txt\n",
      "data/dataset/text_10.txt\n",
      "data/dataset/text_4.txt\n",
      "data/dataset/text_9.txt\n",
      "data/dataset/text_1.txt\n",
      "data/dataset/text_5.txt\n",
      "data/dataset/text_13.txt\n",
      "data/dataset/text_12.txt\n",
      "93093\n"
     ]
    }
   ],
   "source": [
    "x=[]\n",
    "for dir,subdir,files in os.walk(data_dir):\n",
    "    for file in files:\n",
    "        file_path=os.path.join(dir,file)\n",
    "        x+=utils.read_data(file_path)\n",
    "        print(file_path)\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_data={}\n",
    "def add(w1,w2):\n",
    "    key=w1+\"_\"+w2\n",
    "    key2=w2+\"_\"+w1\n",
    "    if key not in aux_data:\n",
    "        aux_data[key]=0\n",
    "        aux_data[key2]=0\n",
    "    aux_data[key]+=1\n",
    "    aux_data[key2]+=1\n",
    "        \n",
    "for line in x:\n",
    "    le=len(line)\n",
    "    for i in range(le):\n",
    "        w1=line[i]\n",
    "        if i+1<le:\n",
    "            w2=line[i+1]\n",
    "            add(w1,w2)\n",
    "        if i+2<le:\n",
    "            w2=line[i+1]\n",
    "            add(w1,w2)\n",
    "utils.dump_pickle(dump=aux_data,filename=\"aux_ds.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def lemmatize(lines):\n",
    "#     new_lines=[]\n",
    "#     lemmatizer=WordNetLemmatizer()\n",
    "#     counter=0\n",
    "#     for line in lines:\n",
    "#         counter+=1\n",
    "#         if(counter%10000==0):\n",
    "#             print(counter)\n",
    "#         new_line=[]\n",
    "#         for word in line:\n",
    "#             new_line.append(lemmatizer.lemmatize(word))\n",
    "#         new_lines.append(new_line)\n",
    "#     return new_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n"
     ]
    }
   ],
   "source": [
    "lemm_x=utils.lemmatize(x)\n",
    "# utils.dump_pickle(lemm_x,\"lemm_x.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_train(x,size,epochs,model,mp=\"\",pret=False):\n",
    "    model_2 = Word2Vec(size=300, min_count=1,workers=4,sg=1)\n",
    "    model_2.build_vocab(x)\n",
    "    total_examples = model_2.corpus_count\n",
    "    print(total_examples)\n",
    "    print(len(list(model_2.wv.vocab.keys())))\n",
    "    print(list(model_2.wv.vocab.keys())[0:10])\n",
    "    total_examples = model_2.corpus_count\n",
    "    model_path=\"sg_emb\"+str(size)+\"_ep\"+str(epochs)+\".pkl\"\n",
    "    if pret:\n",
    "        model_2.intersect_word2vec_format(pretrained_vectors, binary=True, lockf=1.0)\n",
    "        model_path=\"pre_\"+model_path\n",
    "    model_path=mp+model_path\n",
    "    model_2.train(x, total_examples=total_examples, epochs=epochs,report_delay=20)\n",
    "    utils.dump_pickle(model_2,model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Google Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = gensim.models.KeyedVectors.load_word2vec_format(pretrained_vectors, binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x7f7dfca040f0>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93093\n",
      "24271\n",
      "['the', 'lay', 'man', 's', 'sermon', 'upon', 'late', 'storm', 'held', 'forth']\n"
     ]
    }
   ],
   "source": [
    "my_train(lemm_x,300,10,model,\"lemm_\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93093\n",
      "24271\n",
      "['the', 'lay', 'man', 's', 'sermon', 'upon', 'late', 'storm', 'held', 'forth']\n"
     ]
    }
   ],
   "source": [
    "my_train(lemm_x,300,10,model,\"lemm_\",False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93093\n",
      "24271\n",
      "['the', 'lay', 'man', 's', 'sermon', 'upon', 'late', 'storm', 'held', 'forth']\n"
     ]
    }
   ],
   "source": [
    "my_train(lemm_x,200,10,model,\"lemm_\",False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = FastText(min_count=1,workers=4,min_n=1)\n",
    "model2.build_vocab(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93093, 5)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.corpus_count,model2.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.train(x, total_examples=model2.corpus_count, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=\"fsttxt_emb100_ep40.pkl\"\n",
    "utils.dump_pickle(model2,model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/GoogleNews-vectors-negative300.bin.gz'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x7f7dfca040f0>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
