{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize,RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.sentiment.util import mark_negation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename,\"rb\") as f:\n",
    "        dump=pickle.load(f)\n",
    "    return dump\n",
    "\n",
    "def dump_pickle(dump,filename):\n",
    "    with open(filename,\"wb\") as f:\n",
    "        pickle.dump(dump,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file='data/A1_Data/toy.json'\n",
    "dev_file='data/A1_Data/toy.json'\n",
    "tokenized_train=\"tokenized_train.pkl\"\n",
    "tokenized_dev=\"tokenized_dev.pkl\"\n",
    "log=10**5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file='data/A1_Data/train.json'\n",
    "dev_file='data/A1_Data/dev.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    x=[]\n",
    "    y=[]\n",
    "    \n",
    "    file_reader=open(filename,\"r\")\n",
    "    for line in file_reader:\n",
    "        mapping=json.loads(line)\n",
    "        x.append(mapping['review'])\n",
    "        y.append(mapping['ratings'])\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 200000\n"
     ]
    }
   ],
   "source": [
    "train_sent,train_y=read_data(train_file)\n",
    "dev_sent,dev_y=read_data(dev_file)\n",
    "print(len(train_sent),len(dev_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(data):\n",
    "    tokenized=[]\n",
    "    for i in range(len(data)):\n",
    "        if(i%100000==0):\n",
    "            print(i)\n",
    "        tokenized.append(word_tokenize(data[i]))\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "0\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "# train_x=tokenize_data(train_sent)\n",
    "# dev_x=tokenize_data(dev_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(train_x),train_x[0],type(train_x),tokenized_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump_pickle(train_x,tokenized_train)\n",
    "# dump_pickle(dev_x,tokenized_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'ANkesh', '.', 'I', 'am', '21', ':', ')']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"I am ANkesh. I am 21 :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=load_pickle(tokenized_train)\n",
    "dev_x=load_pickle(tokenized_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 200000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_x),len(dev_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "#Consider Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_stopwords_lemmatize(data):\n",
    "    new_data=[]\n",
    "    count=0\n",
    "    for line in data:\n",
    "        count+=1\n",
    "        if(count%log==0):\n",
    "            print(count)\n",
    "        line=mark_negation(line,shallow=True,double_neg_flip=True)\n",
    "        new_line=[]\n",
    "        for word in line:\n",
    "            lemmatized_word=lemmatizer.lemmatize(word)\n",
    "            if lemmatized_word not in stop_words:\n",
    "                new_line.append(lemmatized_word)\n",
    "        new_data.append(new_line)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "100000\n",
      "200000\n"
     ]
    }
   ],
   "source": [
    "# train_x=rem_stopwords_lemmatize(train_x)\n",
    "# dev_x=rem_stopwords_lemmatize(dev_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump_pickle(train_x,'cleaned_train.pkl')\n",
    "# dump_pickle(dev_x,'cleaned_dev.pkl')mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_comp=load_pickle('cleaned_train.pkl')\n",
    "dev_x_comp=load_pickle('cleaned_dev.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut=50000\n",
    "# train_x=train_x_comp[0:cut]\n",
    "# train_y=train_y[0:cut]\n",
    "# dev_x=dev_x_comp[0:cut]\n",
    "# dev_y=dev_y[0:cut]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 200000\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(train_x_comp),len(dev_x_comp))\n",
    "print(isinstance(train_x_comp[0][0],str))\n",
    "for i in train_x_comp:\n",
    "    for j in i:\n",
    "        if(isinstance(j,str)==False):\n",
    "            print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper:https://www.ritchieng.com/machine-learning-multinomial-naive-bayes-vectorization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer=<function temp at 0x7f071b8ba0d0>, binary=False,\n",
      "        decode_error='strict', dtype=<class 'numpy.int64'>,\n",
      "        encoding='utf-8', input='content', lowercase=True, max_df=1.0,\n",
      "        max_features=None, min_df=1, ngram_range=(1, 1), preprocessor=None,\n",
      "        stop_words=None, strip_accents=None,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(analyzer=temp)\n",
    "print(vect)\n",
    "train_x_features = vect.fit_transform(train_x_comp)\n",
    "dev_x_features = vect.transform(dev_x_comp)\n",
    "# print(train_x_features,dev_x_features)0.5605887140074481"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<1000000x663426 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 64420201 stored elements in Compressed Sparse Row format>,\n",
       " <200000x663426 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 12809187 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_features,dev_x_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(gold,pred):\n",
    "    length=len(gold)\n",
    "    return np.sum(gold==pred)*1.0/length\n",
    "def get_Fscore(gold,pred):\n",
    "    return (f1_score(gold, pred, average='macro'),f1_score(gold, pred, average='micro'))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultinomialNaiveBayes(X,Y,dev_X,dev_Y):\n",
    "    nb = MultinomialNB()\n",
    "    %time nb.fit(X,Y)\n",
    "    pred_X=nb.predict(X)\n",
    "    pred_dev=nb.predict(dev_X)\n",
    "    print(get_accuracy(Y,pred_X))\n",
    "    print(get_accuracy(dev_y,pred_dev))\n",
    "    print(get_Fscore(Y,pred_X))\n",
    "    print(get_Fscore(dev_y,pred_dev))\n",
    "    print(mean_squared_error(dev_y,pred_dev))\n",
    "    print(mean_squared_error(dev_y,pred_dev))\n",
    "    print(confusion_matrix(Y, pred_X))\n",
    "    print(confusion_matrix(dev_y, pred_dev))\n",
    "    return nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(X,Y,dev_X,dev_Y):\n",
    "    clf = LinearSVC(random_state=0, tol=1e-5,max_iter=1000)\n",
    "    %time clf.fit(X,Y)\n",
    "    pred_X=clf.predict(X)\n",
    "    pred_dev=clf.predict(dev_X)\n",
    "    print(get_accuracy(Y,pred_X))\n",
    "    print(get_accuracy(dev_y,pred_dev))\n",
    "    print(get_Fscore(Y,pred_X))\n",
    "    print(get_Fscore(dev_y,pred_dev))\n",
    "    print(mean_squared_error(dev_y,pred_dev))\n",
    "    print(confusion_matrix(Y, pred_X))\n",
    "    print(confusion_matrix(dev_y, pred_dev))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_obj=SelectKBest(chi2, k=1000)\n",
    "X_new = selection_obj.fit_transform(train_x_features, train_y)\n",
    "dev_new=selection_obj.transform(dev_x_features)\n",
    "X_new,dev_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsiticRegression(X,Y,dev_X,dev_Y):\n",
    "    log_reg = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial', max_iter=100).fit(X, Y)\n",
    "    pred_X=log_reg.predict(X)\n",
    "    pred_dev=log_reg.predict(dev_X)\n",
    "    print(get_accuracy(Y,pred_X))\n",
    "    print(get_accuracy(dev_y,pred_dev))\n",
    "    print(get_Fscore(Y,pred_X))\n",
    "    print(get_Fscore(dev_y,pred_dev))\n",
    "    print(mean_squared_error(dev_y,pred_dev))\n",
    "    print(confusion_matrix(Y, pred_X))\n",
    "    print(confusion_matrix(dev_y, pred_dev))\n",
    "    return log_reg "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearRegression(X,Y,dev_X,dev_Y):\n",
    "    reg = LinearRegression().fit(X, Y)\n",
    "    pred_X=reg.predict(X)\n",
    "    pred_dev=reg.predict(dev_X)\n",
    "    print(mean_squared_error(dev_y,pred_dev))\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 663426)\n",
      "20000\n",
      "(20000, 663426)\n",
      "20000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [200000, 20000]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-9ad624ff2662>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcut\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcut\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcut\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdev_x_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcut\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdev_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcut\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-387cffd9ebb9>\u001b[0m in \u001b[0;36mlinearRegression\u001b[0;34m(X, Y, dev_X, dev_Y)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpred_X\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpred_dev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/microsoft_AI/lib/python3.6/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \"\"\"\n\u001b[1;32m    238\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 239\u001b[0;31m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "\u001b[0;32m~/anaconda3/envs/microsoft_AI/lib/python3.6/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \"\"\"\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/microsoft_AI/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 235\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [200000, 20000]"
     ]
    }
   ],
   "source": [
    "cut=20000\n",
    "linearRegression(train_x_features[0:cut],train_y[0:cut],dev_x_features[0:cut],dev_y[0:cut])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (microsoft_AI)",
   "language": "python",
   "name": "microsoft_ai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
