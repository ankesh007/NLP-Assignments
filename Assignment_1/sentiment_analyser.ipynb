{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize,RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.sentiment.util import mark_negation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import linear_model\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(filename):\n",
    "    with open(filename,\"rb\") as f:\n",
    "        dump=pickle.load(f)\n",
    "    return dump\n",
    "\n",
    "def dump_pickle(dump,filename):\n",
    "    with open(filename,\"wb\") as f:\n",
    "        pickle.dump(dump,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file='data/A1_Data/toy.json'\n",
    "dev_file='data/A1_Data/toy.json'\n",
    "tokenized_train=\"tokenized_train.pkl\"\n",
    "tokenized_dev=\"tokenized_dev.pkl\"\n",
    "log=10**5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file='data/A1_Data/train.json'\n",
    "dev_file='data/A1_Data/dev.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    x=[]\n",
    "    y=[]\n",
    "    \n",
    "    file_reader=open(filename,\"r\")\n",
    "    for line in file_reader:\n",
    "        mapping=json.loads(line)\n",
    "        x.append(mapping['review'])\n",
    "        y.append(mapping['ratings'])\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 200000\n"
     ]
    }
   ],
   "source": [
    "train_sent,train_y=read_data(train_file)\n",
    "dev_sent,dev_y=read_data(dev_file)\n",
    "print(len(train_sent),len(dev_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    cleaned_data=[]\n",
    "    count=0\n",
    "    for line in data:\n",
    "        count+=1\n",
    "        if(count%log==0):\n",
    "            print(count)\n",
    "        purge=line\n",
    "        purge=re.sub(\"((:\\))|(:-\\)))\",\"good\",purge)\n",
    "        purge=re.sub(\"((:D)|(:-\\)\\)|(:-D)))\",\"very good\",purge)\n",
    "        purge=re.sub(\"((:\\())\",\"bad\",purge)\n",
    "        purge=re.sub(\"((:p))\",\"tricky\",purge)\n",
    "        purge=re.sub(\"((,)|(\\n))\",\" \",purge)\n",
    "        purge = \" \".join(mark_negation(nltk.word_tokenize(purge), double_neg_flip=True, shallow=True))        \n",
    "        cleaned_data.append(purge)\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()\n",
    "counter=0\n",
    "def lemmatize_tokenize(text):\n",
    "    global counter\n",
    "    counter+=1\n",
    "    if(counter%log==0):\n",
    "        print(counter)\n",
    "    word_arr=[]\n",
    "    for word in word_tokenize(text):\n",
    "        word_arr.append(lemmatizer.lemmatize(word))\n",
    "    return word_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(analyzer='word',ngram_range=(1,2),stop_words='english',max_df = 0.85,tokenizer = lemmatize_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_train_data=clean_data(train_sent)\n",
    "# cleaned_dev_data=clean_data(dev_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump_pickle(cleaned_train_data,\"cleaned_train.pkl\")\n",
    "# dump_pickle(cleaned_dev_data,\"cleaned_dev.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loader Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=load_pickle(\"cleaned_train.pkl\")\n",
    "dev_x=load_pickle(\"cleaned_dev.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 200000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_x),len(dev_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(tokenizer = lemmatize_tokenize,analyzer='word',ngram_range=(1,2),stop_words='english',max_df = 0.85)\n",
    "# vectorizer.fit(train_x)\n",
    "# train_x_tfidf=vectorizer.transform(train_x)\n",
    "# dev_x_tfidf=vectorizer.transform(dev_x)\n",
    "# train_x_tfidf,dev_x_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump_pickle(train_x_tfidf,'train_tfidf.pkl')\n",
    "# dump_pickle(dev_x_tfidf,'dev_tfidf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 11357601) (200000, 11357601)\n"
     ]
    }
   ],
   "source": [
    "train_x_tfidf=None\n",
    "dev_x_tfidf=None\n",
    "train_x_tfidf=load_pickle('train_tfidf.pkl')\n",
    "dev_x_tfidf=load_pickle('dev_tfidf.pkl')\n",
    "print(train_x_tfidf.shape,dev_x_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper:https://www.ritchieng.com/machine-learning-multinomial-naive-bayes-vectorization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(gold,pred):\n",
    "    length=len(gold)\n",
    "    return np.sum(gold==pred)*1.0/length\n",
    "def get_Fscore(gold,pred):\n",
    "    return (f1_score(gold, pred, average='macro'),f1_score(gold, pred, average='micro'))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Fitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GeneralFitter(X,Y,obj):\n",
    "    %time obj.fit(X,Y)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb=MultinomialNB()\n",
    "# clf = LinearSVC(random_state=0, tol=1e-5,verbose = 5,max_iter=1000,class_weight='balanced')\n",
    "# log_reg = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial', max_iter=100)\n",
    "# reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(X,Y,dev_X,dev_Y,obj):\n",
    "    pred_X=obj.predict(X)\n",
    "    pred_dev=obj.predict(dev_X)\n",
    "    print(get_accuracy(Y,pred_X))\n",
    "    print(get_accuracy(dev_y,pred_dev))\n",
    "    print(get_Fscore(Y,pred_X))\n",
    "    print(get_Fscore(dev_y,pred_dev))\n",
    "    print(mean_squared_error(dev_y,pred_dev))\n",
    "    print(confusion_matrix(Y, pred_X))\n",
    "    print(confusion_matrix(dev_y, pred_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reg_stats(X,Y,dev_X,dev_Y,obj):\n",
    "    pred_X=obj.predict(X)\n",
    "    pred_dev=obj.predict(dev_X)\n",
    "    pred_X[pred_X[:]<1]=1\n",
    "    pred_X[pred_X[:]>5]=5\n",
    "    pred_dev[pred_dev[:]<1]=1\n",
    "    pred_dev[pred_dev[:]>5]=5\n",
    "    \n",
    "    print(mean_squared_error(Y,pred_X))    \n",
    "    print(mean_squared_error(dev_y,pred_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<1000000x200000 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 85817275 stored elements in Compressed Sparse Row format>,\n",
       " <200000x200000 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 17144738 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selection_obj_tfidf=SelectKBest(chi2, k=200000)\n",
    "filtered_train_tfidf = selection_obj_tfidf.fit_transform(train_x_tfidf, train_y)\n",
    "filtered_dev_tfidf=selection_obj_tfidf.transform(dev_x_tfidf)\n",
    "filtered_train_tfidf,filtered_dev_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 200000)\n"
     ]
    }
   ],
   "source": [
    "print(filtered_dev_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 20min 33s, sys: 2.23 s, total: 1h 20min 35s\n",
      "Wall time: 23min 55s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, normalize=False)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = LinearRegression(n_jobs=-1)\n",
    "GeneralFitter(filtered_train_tfidf,train_y,reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43909863945742583\n",
      "0.5550085894022871\n"
     ]
    }
   ],
   "source": [
    "get_reg_stats(filtered_train_tfidf,train_y,filtered_dev_tfidf,dev_y,reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.4 s, sys: 452 ms, total: 35.8 s\n",
      "Wall time: 34.2 s\n",
      "0.48160461722579856\n",
      "0.5193662628538688\n"
     ]
    }
   ],
   "source": [
    "ridge=linear_model.Ridge(alpha=2)\n",
    "GeneralFitter(filtered_train_tfidf,train_y,ridge)\n",
    "get_reg_stats(filtered_train_tfidf,train_y,filtered_dev_tfidf,dev_y,ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge=linear_model.Ridge(alpha=2)\n",
    "GeneralFitter(train_x_tfidf,train_y,ridge)\n",
    "get_reg_stats(train_x_tfidf,train_y,dev_x_tfidf,dev_y,ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (microsoft_AI)",
   "language": "python",
   "name": "microsoft_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
