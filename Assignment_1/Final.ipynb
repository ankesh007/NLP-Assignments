{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize,RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.sentiment.util import mark_negation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import linear_model\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename,\"rb\") as f:\n",
    "        dump=pickle.load(f)\n",
    "    return dump\n",
    "\n",
    "def dump_pickle(dump,filename):\n",
    "    with open(filename,\"wb\") as f:\n",
    "        pickle.dump(dump,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file='data/A1_Data/toy.json'\n",
    "dev_file='data/A1_Data/toy.json'\n",
    "tokenized_train=\"tokenized_train.pkl\"\n",
    "tokenized_dev=\"tokenized_dev.pkl\"\n",
    "log=10**5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file='data/A1_Data/train.json'\n",
    "dev_file='data/A1_Data/dev.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    x=[]\n",
    "    y=[]\n",
    "    \n",
    "    file_reader=open(filename,\"r\")\n",
    "    for line in file_reader:\n",
    "        mapping=json.loads(line)\n",
    "        x.append(mapping['review'])\n",
    "        y.append(mapping['ratings'])\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 200000\n"
     ]
    }
   ],
   "source": [
    "train_sent,train_y=read_data(train_file)\n",
    "dev_sent,dev_y=read_data(dev_file)\n",
    "print(len(train_sent),len(dev_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(data):\n",
    "    tokenized=[]\n",
    "    for i in range(len(data)):\n",
    "        if(i%100000==0):\n",
    "            print(i)\n",
    "        tokenized.append(word_tokenize(data[i]))\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "0\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "# train_x=tokenize_data(train_sent)\n",
    "# dev_x=tokenize_data(dev_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(train_x),train_x[0],type(train_x),tokenized_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump_pickle(train_x,tokenized_train)\n",
    "# dump_pickle(dev_x,tokenized_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'ANkesh', '.', 'I', 'am', '21', ':', ')']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"I am ANkesh. I am 21 :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=load_pickle(tokenized_train)\n",
    "dev_x=load_pickle(tokenized_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 200000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_x),len(dev_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "#Consider Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_stopwords_lemmatize(data):\n",
    "    new_data=[]\n",
    "    count=0\n",
    "    for line in data:\n",
    "        count+=1\n",
    "        if(count%log==0):\n",
    "            print(count)\n",
    "        line=mark_negation(line,shallow=True,double_neg_flip=True)\n",
    "        new_line=[]\n",
    "        for word in line:\n",
    "            lemmatized_word=lemmatizer.lemmatize(word)\n",
    "            if lemmatized_word not in stop_words:\n",
    "                new_line.append(lemmatized_word)\n",
    "        new_data.append(new_line)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "100000\n",
      "200000\n"
     ]
    }
   ],
   "source": [
    "# train_x=rem_stopwords_lemmatize(train_x)\n",
    "# dev_x=rem_stopwords_lemmatize(dev_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump_pickle(train_x,'cleaned_train.pkl')\n",
    "# dump_pickle(dev_x,'cleaned_dev.pkl')mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_comp=load_pickle('cleaned_train.pkl')\n",
    "dev_x_comp=load_pickle('cleaned_dev.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut=50000\n",
    "# train_x=train_x_comp[0:cut]\n",
    "# train_y=train_y[0:cut]\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.datasets import make_regressiondev_x=dev_x_comp[0:cut]\n",
    "# dev_y=dev_y[0:cut]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 200000\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(train_x_comp),len(dev_x_comp))\n",
    "print(isinstance(train_x_comp[0][0],str))\n",
    "for i in train_x_comp:\n",
    "    for j in i:\n",
    "        if(isinstance(j,str)==False):\n",
    "            print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper:https://www.ritchieng.com/machine-learning-multinomial-naive-bayes-vectorization/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer=<function temp at 0x7ff0b06f7d90>, binary=False,\n",
      "        decode_error='strict', dtype=<class 'numpy.int64'>,\n",
      "        encoding='utf-8', input='content', lowercase=True, max_df=0.85,\n",
      "        max_features=None, min_df=1, ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=None, strip_accents=None,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, vocabulary=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<1000000x663425 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 63467831 stored elements in Compressed Sparse Row format>,\n",
       " <200000x663425 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 12618646 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def temp(x):\n",
    "    return x\n",
    "\n",
    "vect = CountVectorizer(analyzer=temp,ngram_range=(1, 2),max_df=0.85)\n",
    "print(vect)\n",
    "train_x_features = vect.fit_transform(train_x_comp)\n",
    "dev_x_features = vect.transform(dev_x_comp)\n",
    "train_x_features,dev_x_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<1000000x663426 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 64420201 stored elements in Compressed Sparse Row format>,\n",
       " <200000x663426 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 12809187 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def temp(x):\n",
    "    return x\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer=temp,ngram_range=(1,2))\n",
    "vectorizer.fit(train_x_comp)\n",
    "train_x_tfidf=vectorizer.transform(train_x_comp)\n",
    "dev_x_tfidf=vectorizer.transform(dev_x_comp)\n",
    "train_x_tfidf,dev_x_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(gold,pred):\n",
    "    length=len(gold)\n",
    "    return np.sum(gold==pred)*1.0/length\n",
    "def get_Fscore(gold,pred):\n",
    "    return (f1_score(gold, pred, average='macro'),f1_score(gold, pred, average='micro'))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Fitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GeneralFitter(X,Y,obj):\n",
    "    %time obj.fit(X,Y)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb=MultinomialNB()\n",
    "clf = LinearSVC(random_state=0, tol=1e-5,verbose = 5,max_iter=1000,class_weight='balanced')\n",
    "log_reg = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial', max_iter=100)\n",
    "reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(X,Y,dev_X,dev_Y,obj):\n",
    "    pred_X=obj.predict(X)\n",
    "    pred_dev=obj.predict(dev_X)\n",
    "    print(get_accuracy(Y,pred_X))\n",
    "    print(get_accuracy(dev_y,pred_dev))\n",
    "    print(get_Fscore(Y,pred_X))\n",
    "    print(get_Fscore(dev_y,pred_dev))\n",
    "    print(mean_squared_error(dev_y,pred_dev))\n",
    "    print(confusion_matrix(Y, pred_X))\n",
    "    print(confusion_matrix(dev_y, pred_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reg_stats(X,Y,dev_X,dev_Y,obj):\n",
    "    pred_X=obj.predict(X)\n",
    "    pred_dev=obj.predict(dev_X)\n",
    "    pred_X[pred_X[:]<1]=1\n",
    "    pred_X[pred_X[:]>5]=5\n",
    "    pred_dev[pred_dev[:]<1]=1\n",
    "    pred_dev[pred_dev[:]>5]=5\n",
    "    \n",
    "    print(mean_squared_error(Y,pred_X))    \n",
    "    print(mean_squared_error(dev_y,pred_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<1000000x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 33637287 stored elements in Compressed Sparse Row format>,\n",
       " <200000x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 6732487 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selection_obj=SelectKBest(chi2, k=1000)\n",
    "filtered_train = selection_obj.fit_transform(train_x_features, train_y)\n",
    "filtered_dev=selection_obj.transform(dev_x_features)\n",
    "filtered_train,filtered_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<1000000x20000 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 59052593 stored elements in Compressed Sparse Row format>,\n",
       " <200000x20000 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 11816937 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selection_obj_tfidf=SelectKBest(chi2, k=20000)\n",
    "filtered_train_tfidf = selection_obj_tfidf.fit_transform(train_x_tfidf, train_y)\n",
    "filtered_dev_tfidf=selection_obj_tfidf.transform(dev_x_tfidf)\n",
    "filtered_train_tfidf,filtered_dev_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(filtered_dev_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.16 s, sys: 196 ms, total: 2.36 s\n",
      "Wall time: 1.26 s\n",
      "0.651809\n",
      "0.613925\n",
      "(0.5790082449946594, 0.651809)\n",
      "(0.5249208163419372, 0.613925)\n",
      "1.0594\n",
      "[[112653  22560   7604   3551   3792]\n",
      " [ 20777  29410  19249   8788   3046]\n",
      " [ 10846   8410  48371  34821   8133]\n",
      " [  8539   3915  15745 133360  58069]\n",
      " [ 19802   2389   7104  81051 328015]]\n",
      "[[21894  4700  1567   794   834]\n",
      " [ 4413  4439  4715  1868   704]\n",
      " [ 2263  2147  7518  8432  1756]\n",
      " [ 1791   758  3929 24605 12923]\n",
      " [ 4242   457  1307 17615 64329]]\n"
     ]
    }
   ],
   "source": [
    "nb_cv=MultinomialNB()\n",
    "GeneralFitter(train_x_features,train_y,nb_cv)\n",
    "get_stats(train_x_features,train_y,dev_x_features,dev_y,nb_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_tf=MultinomialNB()\n",
    "# GeneralFitter(train_x_tfidf,train_y,nb_tf)\n",
    "# get_stats(train_x_tfidf,train_y,dev_x_tfidf,dev_y,nb_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 5s, sys: 13.1 s, total: 9min 18s\n",
      "Wall time: 5min 59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankesh/anaconda3/envs/microsoft_AI/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=None, penalty='l2', random_state=0, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_cv = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial', max_iter=100)\n",
    "GeneralFitter(train_x_features,train_y,log_reg_cv)\n",
    "get_stats(train_x_features,train_y,dev_x_features,dev_y,log_reg_cv)n_jobs=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.685439\n",
      "0.67845\n",
      "(0.5883533698893385, 0.685439)\n",
      "(0.5775157339302541, 0.67845)\n",
      "0.72456\n",
      "[[124726  11827   3640   1953   8014]\n",
      " [ 26740  26873  16017   5790   5850]\n",
      " [  8869  12462  42375  32380  14495]\n",
      " [  3217   2633  15853  99295  98630]\n",
      " [  4201   1000   2854  38136 392170]]\n",
      "[[24476  2481   770   405  1657]\n",
      " [ 5473  5066  3284  1173  1143]\n",
      " [ 1813  2620  8201  6578  2904]\n",
      " [  634   536  3227 19531 20078]\n",
      " [  892   196   585  7861 78416]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankesh/anaconda3/envs/microsoft_AI/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 36s, sys: 11 s, total: 7min 47s\n",
      "Wall time: 3min 41s\n",
      "0.695181\n",
      "0.685575\n",
      "(0.6042086654923555, 0.695181)\n",
      "(0.5910936223992282, 0.685575)\n",
      "0.652865\n",
      "[[127398  11249   4353   2167   4993]\n",
      " [ 26789  26095  18444   5884   4058]\n",
      " [  8576  10855  47807  33007  10336]\n",
      " [  3393   2127  18064 112784  83260]\n",
      " [  4633    718   3612  48301 381097]]\n",
      "[[25040  2367   911   458  1013]\n",
      " [ 5457  4935  3804  1166   777]\n",
      " [ 1767  2334  9182  6754  2079]\n",
      " [  682   439  3753 21963 17169]\n",
      " [  964   160   777 10054 75995]]\n"
     ]
    }
   ],
   "source": [
    "log_reg_tf=LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial', max_iter=100)\n",
    "GeneralFitter(train_x_tfidf,train_y,log_reg_tf)\n",
    "get_stats(train_x_tfidf,train_y,dev_x_tfidf,dev_y,log_reg_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_pickle(log_reg_tf,\"log_reg_tf.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]CPU times: user 2min 28s, sys: 107 ms, total: 2min 28s\n",
      "Wall time: 2min 30s\n",
      "0.661501\n",
      "0.66069\n",
      "(0.577770452154133, 0.661501)\n",
      "(0.576785221322023, 0.66069)\n",
      "0.79196\n",
      "[[123976  17182   3720   1293   3989]\n",
      " [ 26136  32515  15562   3878   3179]\n",
      " [  9833  19372  46441  25159   9776]\n",
      " [  5648   7626  28289  97713  80352]\n",
      " [  9501   4160   8976  54868 360856]]\n",
      "[[24531  3417   768   281   792]\n",
      " [ 5245  6465  3042   770   617]\n",
      " [ 1984  3917  9260  4994  1961]\n",
      " [ 1149  1502  5655 19629 16071]\n",
      " [ 1920   876  1837 11064 72253]]\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(random_state=0, tol=1e-5,verbose = 5,max_iter=1000,class_weight='balanced')\n",
    "GeneralFitter(filtered_train_tfidf,train_y,svm)\n",
    "get_stats(filtered_train_tfidf,train_y,filtered_dev_tfidf,dev_y,svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]CPU times: user 3min 37s, sys: 204 ms, total: 3min 37s\n",
      "Wall time: 3min 45s\n",
      "0.811779\n",
      "0.66543\n",
      "(0.7812145589227063, 0.811779)\n",
      "(0.5818336961995876, 0.66543)\n",
      "0.657005\n",
      "[[136987   9501   2438    514    720]\n",
      " [ 12369  59743   7159   1330    669]\n",
      " [  4658   9498  80853  12052   3520]\n",
      " [  2051   4115  16965 147496  49001]\n",
      " [  2536   2090   6556  40479 386700]]\n",
      "[[24145  3877  1058   275   434]\n",
      " [ 4946  6379  3612   816   386]\n",
      " [ 1681  4066  9603  5380  1386]\n",
      " [  694  1589  6203 20748 14772]\n",
      " [ 1041   758  1956 11984 72211]]\n"
     ]
    }
   ],
   "source": [
    "svm_full = LinearSVC(random_state=0, tol=1e-5,verbose = 5,max_iter=1000,class_weight='balanced')\n",
    "GeneralFitter(train_x_tfidf,train_y,svm_full)\n",
    "get_stats(train_x_tfidf,train_y,dev_x_tfidf,dev_y,svm_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14min 45s, sys: 724 ms, total: 14min 46s\n",
      "Wall time: 3min 42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, normalize=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = LinearRegression(n_jobs=-1)\n",
    "GeneralFitter(filtered_train_tfidf,train_y,reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5577533438926271\n",
      "0.5874714013902615\n"
     ]
    }
   ],
   "source": [
    "get_reg_stats(filtered_train_tfidf,train_y,filtered_dev_tfidf,dev_y,reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.9 s, sys: 312 ms, total: 25.2 s\n",
      "Wall time: 23.7 s\n",
      "0.5737841644944517\n",
      "0.5900407214194511\n"
     ]
    }
   ],
   "source": [
    "ridge=linear_model.Ridge(alpha=4)\n",
    "GeneralFitter(filtered_train_tfidf,train_y,ridge)\n",
    "get_reg_stats(filtered_train_tfidf,train_y,filtered_dev_tfidf,dev_y,ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regr = RandomForestRegressor(max_depth=5, random_state=0,n_estimators=10,n_jobs=-1)\n",
    "# GeneralFitter(filtered_train_tfidf,train_y,regr)\n",
    "# get_reg_stats(filtered_train_tfidf,train_y,filtered_dev_tfidf,dev_y,regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'did', \"n't\", 'like', 'movie']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"i didn't like movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'did', \"n't\", ',_NEG', 'like_NEG', 'movie_NEG']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mark_negation(word_tokenize(\"i didn't, like movie\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (microsoft_AI)",
   "language": "python",
   "name": "microsoft_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
