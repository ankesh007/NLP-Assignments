{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize,RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.sentiment.util import mark_negation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectKBest, chi2,f_regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import linear_model\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(filename):\n",
    "    with open(filename,\"rb\") as f:\n",
    "        dump=pickle.load(f)\n",
    "    return dump\n",
    "\n",
    "def dump_pickle(dump,filename):\n",
    "    with open(filename,\"wb\") as f:\n",
    "        pickle.dump(dump,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file='data/A1_Data/toy.json'\n",
    "dev_file='data/A1_Data/toy.json'\n",
    "tokenized_train=\"tokenized_train.pkl\"\n",
    "tokenized_dev=\"tokenized_dev.pkl\"\n",
    "log=10**5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file='data/A1_Data/train.json'\n",
    "dev_file='data/A1_Data/dev.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    x=[]\n",
    "    y=[]\n",
    "    \n",
    "    file_reader=open(filename,\"r\")\n",
    "    for line in file_reader:\n",
    "        mapping=json.loads(line)\n",
    "        x.append(mapping['review'])\n",
    "        y.append(mapping['ratings'])\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 200000\n"
     ]
    }
   ],
   "source": [
    "train_sent,train_y=read_data(train_file)\n",
    "dev_sent,dev_y=read_data(dev_file)\n",
    "print(len(train_sent),len(dev_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    cleaned_data=[]\n",
    "    count=0\n",
    "    for line in data:\n",
    "        count+=1\n",
    "        if(count%log==0):\n",
    "            print(count)\n",
    "        purge=line\n",
    "        purge=re.sub(\"((:\\))|(:-\\)))\",\"good\",purge)\n",
    "        purge=re.sub(\"((:D)|(:-\\)\\)|(:-D)))\",\"very good\",purge)\n",
    "        purge=re.sub(\"((:\\())\",\"bad\",purge)\n",
    "        purge=re.sub(\"((:p))\",\"tricky\",purge)\n",
    "        purge=re.sub(\"((,)|(\\n))\",\" \",purge)\n",
    "        purge = mark_negation(nltk.word_tokenize(purge), double_neg_flip=True, shallow=True)\n",
    "        new_purge=[]\n",
    "        for x in purge:\n",
    "            if x not in new_purge:\n",
    "                new_purge.append(x)\n",
    "        purge=new_purge\n",
    "        purge= \" \".join(purge)\n",
    "        cleaned_data.append(purge)\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()\n",
    "counter=0\n",
    "def lemmatize_tokenize(text):\n",
    "    global counter\n",
    "    counter+=1\n",
    "    if(counter%log==0):\n",
    "        print(counter)\n",
    "    word_arr=[]\n",
    "    for word in word_tokenize(text):\n",
    "        word_arr.append(lemmatizer.lemmatize(word))\n",
    "    return word_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(analyzer='word',ngram_range=(1,2),stop_words='english',max_df = 0.85,tokenizer = lemmatize_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_train_data=clean_data(train_sent)\n",
    "# cleaned_dev_data=clean_data(dev_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump_pickle(cleaned_train_data,\"cleaned_train.pkl\")\n",
    "# dump_pickle(cleaned_dev_data,\"cleaned_dev.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loader Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=load_pickle(\"cleaned_train.pkl\")\n",
    "dev_x=load_pickle(\"cleaned_dev.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 200000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_x),len(dev_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "CPU times: user 19min 53s, sys: 8.53 s, total: 20min 2s\n",
      "Wall time: 20min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<1000000x1000000 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 146157012 stored elements in Compressed Sparse Row format>,\n",
       " <200000x1000000 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 29252923 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=1000000,tokenizer = lemmatize_tokenize,analyzer='word',ngram_range=(1,2))\n",
    "temp=train_x+dev_x\n",
    "%time train_test_x_tfidf=vectorizer.fit_transform(temp)\n",
    "train_x_tfidf=train_test_x_tfidf[0:len(train_x)]\n",
    "dev_x_tfidf=train_test_x_tfidf[len(train_x):]\n",
    "train_x_tfidf,dev_x_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COunt Vectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vectorizer_count = CountVectorizer(max_features=1000000,tokenizer = lemmatize_tokenize,analyzer='word',ngram_range=(1,2),max_df=0.85)\n",
    "# temp=train_x+dev_x\n",
    "# %time train_test_x_count=vectorizer_count.fit_transform(temp)\n",
    "# train_x_count=train_test_x_count[0:len(train_x)]\n",
    "# dev_x_count=train_test_x_count[len(train_x):]\n",
    "# train_x_count,dev_x_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump_pickle(train_x_tfidf,'train_tfidf.pkl')\n",
    "# dump_pickle(dev_x_tfidf,'dev_tfidf.pkl')\n",
    "# dump_pickle(train_test_x_tfidf,'train_dev_tfidf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump_pickle(train_x_count,'train_count.pkl')\n",
    "# dump_pickle(dev_x_count,'dev_count.pkl')\n",
    "# dump_pickle(train_test_x_count,'train_dev_count.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 1000000) (200000, 1000000)\n"
     ]
    }
   ],
   "source": [
    "train_x_tfidf=None\n",
    "dev_x_tfidf=None\n",
    "train_x_tfidf=load_pickle('train_tfidf.pkl')\n",
    "dev_x_tfidf=load_pickle('dev_tfidf.pkl')\n",
    "print(train_x_tfidf.shape,dev_x_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x_count=None\n",
    "# dev_x_count=None\n",
    "# train_x_count=load_pickle('train_tfidf.pkl')\n",
    "# dev_x_count=load_pickle('dev_tfidf.pkl')\n",
    "# print(train_x_count.shape,dev_x_count.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper:https://www.ritchieng.com/machine-learning-multinomial-naive-bayes-vectorization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(gold,pred):\n",
    "    length=len(gold)\n",
    "    return np.sum(gold==pred)*1.0/length\n",
    "def get_Fscore(gold,pred):\n",
    "    return (f1_score(gold, pred, average='macro'),f1_score(gold, pred, average='micro'))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Fitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GeneralFitter(X,Y,obj):\n",
    "    %time obj.fit(X,Y)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb=MultinomialNB()\n",
    "# clf = LinearSVC(random_state=0, tol=1e-5,verbose = 5,max_iter=1000,class_weight='balanced')\n",
    "# log_reg = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial', max_iter=100)\n",
    "# reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(X,Y,dev_X,dev_Y,obj):\n",
    "    pred_X=obj.predict(X)\n",
    "    pred_dev=obj.predict(dev_X)\n",
    "    print(get_accuracy(Y,pred_X))\n",
    "    print(get_accuracy(dev_y,pred_dev))\n",
    "    print(get_Fscore(Y,pred_X))\n",
    "    print(get_Fscore(dev_y,pred_dev))\n",
    "    print(mean_squared_error(dev_y,pred_dev))\n",
    "    print(confusion_matrix(Y, pred_X))\n",
    "    print(confusion_matrix(dev_y, pred_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reg_stats(X,Y,dev_X,dev_Y,obj):\n",
    "    pred_X=obj.predict(X)\n",
    "    pred_dev=obj.predict(dev_X)\n",
    "    pred_X[pred_X[:]<1]=1\n",
    "    pred_X[pred_X[:]>5]=5\n",
    "    pred_dev[pred_dev[:]<1]=1\n",
    "    pred_dev[pred_dev[:]>5]=5\n",
    "    \n",
    "    print(mean_squared_error(Y,pred_X))    \n",
    "    print(mean_squared_error(dev_y,pred_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection_obj_tfidf=SelectKBest(f_regression, k=100000)\n",
    "# filtered_train_tfidf = selection_obj_tfidf.fit_transform(train_x_tfidf, train_y)\n",
    "# filtered_dev_tfidf=selection_obj_tfidf.transform(dev_x_tfidf)\n",
    "# filtered_train_tfidf,filtered_dev_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filtered_dev_tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-f48a8014fe55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_dev_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'filtered_dev_tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "print(filtered_dev_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg = LinearRegression(n_jobs=-1)\n",
    "# GeneralFitter(filtered_train_tfidf,train_y,reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_reg_stats(filtered_train_tfidf,train_y,filtered_dev_tfidf,dev_y,reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.3 s, sys: 392 ms, total: 28.7 s\n",
      "Wall time: 27.1 s\n",
      "0.508253193025252\n",
      "0.55289006341852\n"
     ]
    }
   ],
   "source": [
    "ridge=linear_model.Ridge(alpha=2)\n",
    "GeneralFitter(train_x_tfidf,train_y,ridge)\n",
    "get_reg_stats(train_x_tfidf,train_y,dev_tfidf,dev_y,ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.2 s, sys: 296 ms, total: 25.5 s\n",
      "Wall time: 23.8 s\n",
      "0.533518284326126\n",
      "0.5661441872430603\n"
     ]
    }
   ],
   "source": [
    "ridge=linear_model.Ridge(alpha=4)\n",
    "GeneralFitter(filtered_train_tfidf,train_y,ridge)\n",
    "get_reg_stats(filtered_train_tfidf,train_y,filtered_dev_tfidf,dev_y,ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 40s, sys: 1.27 s, total: 4min 41s\n",
      "Wall time: 4min 37s\n",
      "0.84246\n",
      "0.701565\n",
      "(0.8030076933116147, 0.84246)\n",
      "(0.6132428734442106, 0.701565)\n",
      "0.563635\n",
      "[[140570   4706   2030    862   1992]\n",
      " [ 16092  51477   7590   3371   2740]\n",
      " [  6165   4151  74912  17180   8173]\n",
      " [  1985   1029   5803 159573  51238]\n",
      " [  1607    265   1426  19135 415928]]\n",
      "[[25430  2567   689   313   790]\n",
      " [ 5385  5698  3504   881   671]\n",
      " [ 1714  2431  9421  6587  1963]\n",
      " [  558   364  3144 22678 17262]\n",
      " [  647   108   484  9625 77086]]\n"
     ]
    }
   ],
   "source": [
    "log_reg_tf=LogisticRegression(random_state=0, solver='sag',multi_class='multinomial', max_iter=100,n_jobs=-1)\n",
    "GeneralFitter(train_x_tfidf,train_y,log_reg_tf)\n",
    "get_stats(train_x_tfidf,train_y,dev_x_tfidf,dev_y,log_reg_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import ShuffleSplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=linear_model.Ridge(alpha=2)\n",
    "cv = ShuffleSplit(n_splits=2, test_size=0.25, random_state=0)\n",
    "cv_results = cross_validate(model, X, y, cv=cv,return_train_score=True,return_estimator=True,n_jobs=-1,scoring='neg_mean_squared_error')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (microsoft_AI)",
   "language": "python",
   "name": "microsoft_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
