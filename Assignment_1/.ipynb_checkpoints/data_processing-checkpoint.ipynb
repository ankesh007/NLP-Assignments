{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize,RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.sentiment.util import mark_negation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename,\"rb\") as f:\n",
    "        dump=pickle.load(f)\n",
    "    return dump\n",
    "\n",
    "def dump_pickle(dump,filename):\n",
    "    with open(filename,\"wb\") as f:\n",
    "        pickle.dump(dump,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file='data/A1_Data/toy.json'\n",
    "dev_file='data/A1_Data/toy.json'\n",
    "tokenized_train=\"tokenized_train.pkl\"\n",
    "tokenized_dev=\"tokenized_dev.pkl\"\n",
    "log=10**5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file='data/A1_Data/train.json'\n",
    "dev_file='data/A1_Data/dev.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    x=[]\n",
    "    y=[]\n",
    "    \n",
    "    file_reader=open(filename,\"r\")\n",
    "    for line in file_reader:\n",
    "        mapping=json.loads(line)\n",
    "        x.append(mapping['review'])\n",
    "        y.append(mapping['ratings'])\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 200000\n"
     ]
    }
   ],
   "source": [
    "train_sent,train_y=read_data(train_file)\n",
    "dev_sent,dev_y=read_data(dev_file)\n",
    "print(len(train_sent),len(dev_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(data):\n",
    "    tokenized=[]\n",
    "    for i in range(len(data)):\n",
    "        if(i%100000==0):\n",
    "            print(i)\n",
    "        tokenized.append(word_tokenize(data[i]))\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "0\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "# train_x=tokenize_data(train_sent)\n",
    "# dev_x=tokenize_data(dev_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(train_x),train_x[0],type(train_x),tokenized_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump_pickle(train_x,tokenized_train)\n",
    "# dump_pickle(dev_x,tokenized_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'ANkesh', '.', 'I', 'am', '21', ':', ')']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"I am ANkesh. I am 21 :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=load_pickle(tokenized_train)\n",
    "dev_x=load_pickle(tokenized_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 200000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_x),len(dev_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "#Consider Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_stopwords_lemmatize(data):\n",
    "    new_data=[]\n",
    "    count=0\n",
    "    for line in data:\n",
    "        count+=1\n",
    "        if(count%log==0):\n",
    "            print(count)\n",
    "        line=mark_negation(line,shallow=True,double_neg_flip=True)\n",
    "        new_line=[]\n",
    "        for word in line:\n",
    "            lemmatized_word=lemmatizer.lemmatize(word)\n",
    "            if lemmatized_word not in stop_words:\n",
    "                new_line.append(lemmatized_word)\n",
    "        new_data.append(new_line)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "100000\n",
      "200000\n"
     ]
    }
   ],
   "source": [
    "# train_x=rem_stopwords_lemmatize(train_x)\n",
    "# dev_x=rem_stopwords_lemmatize(dev_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump_pickle(train_x,'cleaned_train.pkl')\n",
    "# dump_pickle(dev_x,'cleaned_dev.pkl')mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_comp=load_pickle('cleaned_train.pkl')\n",
    "dev_x_comp=load_pickle('cleaned_dev.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut=50000\n",
    "# train_x=train_x_comp[0:cut]\n",
    "# train_y=train_y[0:cut]\n",
    "# dev_x=dev_x_comp[0:cut]\n",
    "# dev_y=dev_y[0:cut]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 200000\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(train_x),len(dev_x))\n",
    "print(isinstance(train_x[0][0],str))\n",
    "for i in train_x:\n",
    "    for j in i:\n",
    "        if(isinstance(j,str)==False):\n",
    "            print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper:https://www.ritchieng.com/machine-learning-multinomial-naive-bayes-vectorization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer=<function temp at 0x7f9d1d60f378>, binary=False,\n",
      "        decode_error='strict', dtype=<class 'numpy.int64'>,\n",
      "        encoding='utf-8', input='content', lowercase=True, max_df=1.0,\n",
      "        max_features=None, min_df=1, ngram_range=(1, 1), preprocessor=None,\n",
      "        stop_words=None, strip_accents=None,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(analyzer=temp)\n",
    "print(vect)\n",
    "train_x_features = vect.fit_transform(train_x)\n",
    "dev_x_features = vect.transform(dev_x)\n",
    "# print(train_x_features,dev_x_features)0.5605887140074481"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<1000000x571639 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 80269672 stored elements in Compressed Sparse Row format>,\n",
       " <200000x571639 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 15987910 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_features,dev_x_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(gold,pred):\n",
    "    length=len(gold)\n",
    "    return np.sum(gold==pred)*1.0/length\n",
    "def get_Fscore(gold,pred):\n",
    "    return (f1_score(gold, pred, average='macro'),f1_score(gold, pred, average='micro'))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultinomialNaiveBayes(X,Y,dev_X,dev_Y):\n",
    "    nb = MultinomialNB()\n",
    "    %time nb.fit(X,Y)\n",
    "    pred_X=nb.predict(X)\n",
    "    pred_dev=nb.predict(dev_X)\n",
    "    print(get_accuracy(train_y,pred_X))\n",
    "    print(get_accuracy(dev_y,pred_dev))\n",
    "    print(get_Fscore(train_y,pred_X))\n",
    "    print(get_Fscore(dev_y,pred_dev))\n",
    "    print(mean_squared_error(dev_y,pred_dev))\n",
    "    print(confusion_matrix(train_y, pred_X))\n",
    "    print(confusion_matrix(dev_y, pred_dev))\n",
    "    return nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(X,Y,dev_X,dev_Y):\n",
    "    clf = LinearSVC(random_state=0, tol=1e-5,max_iter=1000)\n",
    "    %time clf.fit(X,Y)\n",
    "    pred_train=clf.predict(X)\n",
    "    pred_dev=clf.predict(dev_new)\n",
    "    print(get_accuracy(Y,pred_dev))\n",
    "    print(get_accuracy(dev_y,pred_dev))\n",
    "    print(get_Fscore(Y,pred_dev))\n",
    "    print(get_Fscore(dev_y,pred_dev))\n",
    "    print(mean_squared_error(dev_y,pred_dev))\n",
    "    print(confusion_matrix(Y, pred_dev))\n",
    "    print(confusion_matrix(dev_y, pred_dev))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_obj=SelectKBest(chi2, k=1000)\n",
    "X_new = selection_obj.fit_transform(train_x_features, train_y)\n",
    "dev_new=selection_obj.transform(dev_x_features)\n",
    "X_new,dev_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_x_train=train_x_features\n",
    "logistic_x_dev=dev_x_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankesh/anaconda3/envs/microsoft_AI/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial', max_iter=100).fit(logistic_x_train, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_log=log_reg.predict(logistic_x_train)\n",
    "dev_pred_log=log_reg.predict(logistic_x_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_accuracy(train_y,train_pred_log))\n",
    "print(get_accuracy(dev_y,dev_pred_log))\n",
    "print(get_Fscore(train_y,train_pred_log))\n",
    "print(get_Fscore(dev_y,dev_pred_log))\n",
    "print(mean_squared_error(dev_y,dev_pred_log))\n",
    "confusion_matrix(train_y, train_pred_log)\n",
    "confusion_matrix(dev_y, dev_pred_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(logistic_x_train, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_reg=reg.predict(logistic_x_train)\n",
    "dev_pred_log=log_reg.predict(logistic_x_dev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (microsoft_AI)",
   "language": "python",
   "name": "microsoft_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
