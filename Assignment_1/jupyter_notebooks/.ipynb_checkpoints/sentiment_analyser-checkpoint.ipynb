{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize,RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.sentiment.util import mark_negation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectKBest, chi2,f_regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import linear_model\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(filename):\n",
    "    with open(filename,\"rb\") as f:\n",
    "        dump=pickle.load(f)\n",
    "    return dump\n",
    "\n",
    "def dump_pickle(dump,filename):\n",
    "    with open(filename,\"wb\") as f:\n",
    "        pickle.dump(dump,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file='../data/A1_Data/toy.json'\n",
    "dev_file='../data/A1_Data/toy.json'\n",
    "tokenized_train=\"tokenized_train.pkl\"\n",
    "tokenized_dev=\"tokenized_dev.pkl\"\n",
    "log=10**5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file='../data/A1_Data/train.json'\n",
    "dev_file='../data/A1_Data/dev.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    x=[]\n",
    "    y=[]\n",
    "    \n",
    "    file_reader=open(filename,\"r\")\n",
    "    for line in file_reader:\n",
    "        mapping=json.loads(line)\n",
    "        x.append(mapping['review'])\n",
    "        y.append(mapping['ratings'])\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/A1_Data/train.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-25f9ad3f5c3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_sent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdev_sent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdev_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-f086fba968da>\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mfile_reader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_reader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mmapping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/A1_Data/train.json'"
     ]
    }
   ],
   "source": [
    "train_sent,train_y=read_data(train_file)\n",
    "dev_sent,dev_y=read_data(dev_file)\n",
    "print(len(train_sent),len(dev_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    cleaned_data=[]\n",
    "    count=0\n",
    "    for line in data:\n",
    "        count+=1\n",
    "        if(count%log==0):\n",
    "            print(count)\n",
    "        purge=line\n",
    "        purge=re.sub(\"((:\\))|(:-\\)))\",\"good\",purge)\n",
    "        purge=re.sub(\"((:D)|(:-\\)\\)|(:-D)))\",\"very good\",purge)\n",
    "        purge=re.sub(\"((:\\())\",\"bad\",purge)\n",
    "        purge=re.sub(\"((:p))\",\"tricky\",purge)\n",
    "        purge=re.sub(\"((,)|(\\n))\",\" \",purge)\n",
    "        purge = mark_negation(nltk.word_tokenize(purge), double_neg_flip=True, shallow=True)\n",
    "        new_purge=[]\n",
    "        for x in purge:\n",
    "            if x not in new_purge:\n",
    "                new_purge.append(x)\n",
    "        purge=new_purge\n",
    "        purge= \" \".join(purge)\n",
    "        cleaned_data.append(purge)\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()\n",
    "counter=0\n",
    "def lemmatize_tokenize(text):\n",
    "    global counter\n",
    "    counter+=1\n",
    "    if(counter%log==0):\n",
    "        print(counter)\n",
    "    word_arr=[]\n",
    "    for word in word_tokenize(text):\n",
    "        word_arr.append(lemmatizer.lemmatize(word))\n",
    "    return word_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(analyzer='word',ngram_range=(1,2),stop_words='english',max_df = 0.85,tokenizer = lemmatize_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_train_data=clean_data(train_sent)\n",
    "# cleaned_dev_data=clean_data(dev_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump_pickle(cleaned_train_data,\"cleaned_train.pkl\")\n",
    "# dump_pickle(cleaned_dev_data,\"cleaned_dev.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loader Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=load_pickle(\"cleaned_train.pkl\")\n",
    "dev_x=load_pickle(\"cleaned_dev.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 200000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_x),len(dev_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(max_features=1000000,tokenizer = lemmatize_tokenize,analyzer='word',ngram_range=(1,2))\n",
    "# temp=train_x+dev_x\n",
    "# %time train_test_x_tfidf=vectorizer.fit_transform(temp)\n",
    "# train_x_tfidf=train_test_x_tfidf[0:len(train_x)]\n",
    "# dev_x_tfidf=train_test_x_tfidf[len(train_x):]\n",
    "# train_x_tfidf,dev_x_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COunt Vectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vectorizer_count = CountVectorizer(max_features=1000000,tokenizer = lemmatize_tokenize,analyzer='word',ngram_range=(1,2),max_df=0.85)\n",
    "# temp=train_x+dev_x\n",
    "# %time train_test_x_count=vectorizer_count.fit_transform(temp)\n",
    "# train_x_count=train_test_x_count[0:len(train_x)]\n",
    "# dev_x_count=train_test_x_count[len(train_x):]\n",
    "# train_x_count,dev_x_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump_pickle(train_x_tfidf,'train_tfidf.pkl')\n",
    "# dump_pickle(dev_x_tfidf,'dev_tfidf.pkl')\n",
    "# dump_pickle(train_test_x_tfidf,'train_dev_tfidf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump_pickle(train_x_count,'train_count.pkl')\n",
    "# dump_pickle(dev_x_count,'dev_count.pkl')\n",
    "# dump_pickle(train_test_x_count,'train_dev_count.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 1000000) (200000, 1000000)\n"
     ]
    }
   ],
   "source": [
    "train_x_tfidf=None\n",
    "dev_x_tfidf=None\n",
    "train_x_tfidf=load_pickle('train_tfidf.pkl')\n",
    "dev_x_tfidf=load_pickle('dev_tfidf.pkl')\n",
    "print(train_x_tfidf.shape,dev_x_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x_count=None\n",
    "# dev_x_count=None\n",
    "# train_x_count=load_pickle('train_tfidf.pkl')\n",
    "# dev_x_count=load_pickle('dev_tfidf.pkl')\n",
    "# print(train_x_count.shape,dev_x_count.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper:https://www.ritchieng.com/machine-learning-multinomial-naive-bayes-vectorization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(gold,pred):\n",
    "    length=len(gold)\n",
    "    return np.sum(gold==pred)*1.0/length\n",
    "def get_Fscore(gold,pred):\n",
    "    return (f1_score(gold, pred, average='macro'),f1_score(gold, pred, average='micro'))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Fitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GeneralFitter(X,Y,obj):\n",
    "    %time obj.fit(X,Y)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb=MultinomialNB()\n",
    "# clf = LinearSVC(random_state=0, tol=1e-5,verbose = 5,max_iter=1000,class_weight='balanced')\n",
    "# log_reg = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial', max_iter=100)\n",
    "# reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(X,Y,dev_X,dev_Y,obj):\n",
    "    pred_X=obj.predict(X)\n",
    "    pred_dev=obj.predict(dev_X)\n",
    "    print(get_accuracy(Y,pred_X))\n",
    "    print(get_accuracy(dev_y,pred_dev))\n",
    "    print(get_Fscore(Y,pred_X))\n",
    "    print(get_Fscore(dev_y,pred_dev))\n",
    "    print(mean_squared_error(dev_y,pred_dev))\n",
    "    print(confusion_matrix(Y, pred_X))\n",
    "    print(confusion_matrix(dev_y, pred_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reg_stats(X,Y,dev_X,dev_Y,obj):\n",
    "    pred_X=obj.predict(X)\n",
    "    pred_dev=obj.predict(dev_X)\n",
    "    pred_X[pred_X[:]<1]=1\n",
    "    pred_X[pred_X[:]>5]=5\n",
    "    pred_dev[pred_dev[:]<1]=1\n",
    "    pred_dev[pred_dev[:]>5]=5\n",
    "    \n",
    "    print(mean_squared_error(Y,pred_X))    \n",
    "    print(mean_squared_error(dev_y,pred_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection_obj_tfidf=SelectKBest(f_regression, k=100000)\n",
    "# filtered_train_tfidf = selection_obj_tfidf.fit_transform(train_x_tfidf, train_y)\n",
    "# filtered_dev_tfidf=selection_obj_tfidf.transform(dev_x_tfidf)\n",
    "# filtered_train_tfidf,filtered_dev_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(filtered_dev_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg = LinearRegression(n_jobs=-1)\n",
    "# GeneralFitter(filtered_train_tfidf,train_y,reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_reg_stats(filtered_train_tfidf,train_y,filtered_dev_tfidf,dev_y,reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 35s, sys: 676 ms, total: 1min 35s\n",
      "Wall time: 1min 33s\n",
      "0.329407640959855\n",
      "0.48566750864398583\n"
     ]
    }
   ],
   "source": [
    "ridge=linear_model.Ridge(alpha=2)\n",
    "GeneralFitter(train_x_tfidf,train_y,ridge)\n",
    "get_reg_stats(train_x_tfidf,train_y,dev_x_tfidf,dev_y,ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 17s, sys: 536 ms, total: 1min 18s\n",
      "Wall time: 1min 16s\n",
      "0.3853526792080213\n",
      "0.4922891647145834\n"
     ]
    }
   ],
   "source": [
    "ridge=linear_model.Ridge(alpha=4)\n",
    "GeneralFitter(train_x_tfidf,train_y,ridge)\n",
    "get_reg_stats(train_x_tfidf,train_y,dev_x_tfidf,dev_y,ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_reg_tf=LogisticRegression(random_state=0, solver='sag',multi_class='multinomial', max_iter=100,n_jobs=-1)\n",
    "# GeneralFitter(train_x_tfidf,train_y,log_reg_tf)\n",
    "# get_stats(train_x_tfidf,train_y,dev_x_tfidf,dev_y,log_reg_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import ShuffleSplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix, hstack,vstack\n",
    "y=train_y+dev_y\n",
    "X=load_pickle(\"train_dev_tfidf.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=linear_model.Ridge(alpha=2)\n",
    "cv = ShuffleSplit(n_splits=6, test_size=0.2, random_state=0)\n",
    "cv_results = cross_validate(model, X, y, cv=cv,return_train_score=True,return_estimator=True,n_jobs=2,scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['fit_time', 'score_time', 'estimator', 'test_score', 'train_score'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.51352092 -0.51527013 -0.51320722 -0.51218628 -0.51438011 -0.5149956 ] Test\n",
      "[-0.3474611  -0.34715635 -0.34749069 -0.34778494 -0.34731724 -0.34726736] Train\n",
      "[102.14355636 107.14799976 106.72960639 120.53314066 105.54618025\n",
      "  99.63062954] Time\n"
     ]
    }
   ],
   "source": [
    "print(cv_results['test_score'],\"Test\")\n",
    "print(cv_results['train_score'],\"Train\")\n",
    "print(cv_results['fit_time'],\"Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Ridge(alpha=2, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001), Ridge(alpha=2, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001), Ridge(alpha=2, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001), Ridge(alpha=2, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001), Ridge(alpha=2, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001), Ridge(alpha=2, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001))\n"
     ]
    }
   ],
   "source": [
    "print(cv_results['estimator'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (microsoft_AI)",
   "language": "python",
   "name": "microsoft_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
